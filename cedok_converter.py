#!/usr/bin/env python3
"""
ÄŒedok URL Konvertor
Automaticky extrahuje data z ÄŒedok URL, vytvoÅ™Ã­ affiliate link, zkrÃ¡tÃ­ pÅ™es Bitly
a vygeneruje Å¡ablony pro kanÃ¡l i web.
"""

import requests
from bs4 import BeautifulSoup
import urllib.parse
import sys
import re

# Konfigurace
BITLY_TOKEN = '50e591e73398864b51928b3443ab03d817af94d0'
AFFILIATE_PREFIX = 'https://www.jdoqocy.com/click-100430731-15693379'

def shorten_with_bitly(long_url):
    """ZkrÃ¡tÃ­ URL pÅ™es Bitly API"""
    try:
        response = requests.post(
            'https://api-ssl.bitly.com/v4/shorten',
            headers={
                'Authorization': f'Bearer {BITLY_TOKEN}',
                'Content-Type': 'application/json',
            },
            json={
                'long_url': long_url,
                'domain': 'bit.ly'
            }
        )
        
        if response.status_code in [200, 201]:
            return response.json()['link']
        else:
            print(f"âš ï¸  Bitly chyba: {response.status_code}")
            return long_url
    except Exception as e:
        print(f"âš ï¸  Chyba pÅ™i zkracovÃ¡nÃ­: {e}")
        return long_url

def extract_info_from_url(url):
    """Extrahuje zÃ¡kladnÃ­ info z URL struktury"""
    info = {
        'hotel_name': '',
        'destination': '',
        'flag': 'ğŸŒ',
        'days': 8,
        'date': '',
        'stars': 4,
        'meals': 'All Inclusive',
        'price': '0 KÄ'
    }
    
    try:
        # DekÃ³dovÃ¡nÃ­ URL
        decoded_url = urllib.parse.unquote(url)
        
        # Extrakce nÃ¡zvu hotelu z path
        hotel_match = re.search(r'/hotel-([^,]+)', decoded_url)
        if hotel_match:
            hotel_parts = hotel_match.group(1).replace('(', '').replace(')', '').split('-')
            info['hotel_name'] = ' '.join(word.capitalize() for word in hotel_parts)
        
        # Detekce hvÄ›zdiÄek podle znaÄky hotelu
        hotel_name_lower = info['hotel_name'].lower()
        luxury_brands = ['hilton', 'marriott', 'hyatt', 'intercontinental', 'shangri-la', 'four seasons', 'ritz-carlton', 'waldorf', 'st. regis', 'aryaduta', 'grand hyatt', 'jw marriott', 'park hyatt', 'sofitel', 'kempinski', 'mandarin oriental', 'peninsula', 'raffles', 'oberoi', 'taj']
        upper_brands = ['doubletree', 'courtyard', 'sheraton', 'westin', 'radisson', 'novotel', 'pullman', 'renaissance', 'crowne plaza', 'mercure']
        mid_brands = ['holiday inn', 'ibis', 'best western', 'ramada', 'comfort inn', 'quality inn']
        
        if any(brand in hotel_name_lower for brand in luxury_brands):
            info['stars'] = 5
        elif any(brand in hotel_name_lower for brand in upper_brands):
            info['stars'] = 4
        elif any(brand in hotel_name_lower for brand in mid_brands):
            info['stars'] = 3
        
        # Extrakce destinace
        dest_match = re.search(r'/dovolena/([^/]+)/', decoded_url)
        if dest_match:
            info['destination'] = dest_match.group(1).capitalize()
        
        # Mapa vlajek
        country_flags = {
            'egypt': 'ğŸ‡ªğŸ‡¬', 'recko': 'ğŸ‡¬ğŸ‡·', 'grecko': 'ğŸ‡¬ğŸ‡·', 'spanelsko': 'ğŸ‡ªğŸ‡¸',
            'turecko': 'ğŸ‡¹ğŸ‡·', 'chorvatsko': 'ğŸ‡­ğŸ‡·', 'italie': 'ğŸ‡®ğŸ‡¹', 'bulharsko': 'ğŸ‡§ğŸ‡¬',
            'kypr': 'ğŸ‡¨ğŸ‡¾', 'tunisko': 'ğŸ‡¹ğŸ‡³', 'maroko': 'ğŸ‡²ğŸ‡¦', 'oman': 'ğŸ‡´ğŸ‡²',
            'dubaj': 'ğŸ‡¦ğŸ‡ª', 'uae': 'ğŸ‡¦ğŸ‡ª', 'thajsko': 'ğŸ‡¹ğŸ‡­', 'maledivy': 'ğŸ‡²ğŸ‡»',
            'zanzibar': 'ğŸ‡¹ğŸ‡¿', 'mexiko': 'ğŸ‡²ğŸ‡½', 'dominikana': 'ğŸ‡©ğŸ‡´', 'kuba': 'ğŸ‡¨ğŸ‡º'
        }
        
        dest_lower = info['destination'].lower()
        info['flag'] = country_flags.get(dest_lower, 'ğŸŒ')
        
    except Exception as e:
        print(f"âš ï¸  Chyba pÅ™i extrakci z URL: {e}")
    
    return info

def scrape_cedok_page(url):
    """StÃ¡hne a parsuje ÄŒedok strÃ¡nku pro zÃ­skÃ¡nÃ­ pÅ™esnÃ½ch dat"""
    info = extract_info_from_url(url)
    
    try:
        print("ğŸ“¥ Stahuji data z ÄŒedok...")
        
        # PÅ™idÃ¡me User-Agent aby nÃ¡s web nepovaÅ¾oval za bota
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            print(f"âš ï¸  NepodaÅ™ilo se stÃ¡hnout strÃ¡nku (status: {response.status_code})")
            print("â„¹ï¸  PouÅ¾iji zÃ¡kladnÃ­ data z URL...")
            return info
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # HledÃ¡nÃ­ ceny
        price_patterns = [
            {'class': re.compile(r'price', re.I)},
            {'class': re.compile(r'amount', re.I)},
            {'itemprop': 'price'},
            {'data-price': True}
        ]
        
        for pattern in price_patterns:
            price_elem = soup.find(['span', 'div', 'p'], pattern)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                # Extrakce ÄÃ­sel z textu
                price_match = re.search(r'(\d[\d\s]*)', price_text)
                if price_match:
                    price_num = price_match.group(1).replace(' ', '')
                    info['price'] = f"{price_num} KÄ"
                    print(f"âœ… Cena nalezena: {info['price']}")
                    break
        
        # HledÃ¡nÃ­ hvÄ›zdiÄek - vylepÅ¡enÃ©
        # HledÃ¡me pÅ™Ã­mo ikony s role="listitem" (ÄŒedok pouÅ¾Ã­vÃ¡ tento atribut pro hodnocenÃ­)
        stars_found = False
        
        # Najdeme vÅ¡echny hvÄ›zdiÄky s role="listitem"
        star_icons = soup.find_all('i', {'class': re.compile(r'icon.*star', re.I), 'role': 'listitem'})
        if star_icons and len(star_icons) <= 5:
            info['stars'] = len(star_icons)
            print(f"âœ… HodnocenÃ­ (z ikon s role=listitem): {info['stars']}â­")
            stars_found = True
        
        # Fallback - pokud nenajdeme s role=listitem, zkusÃ­me najÃ­t v parent containeru
        if not stars_found:
            # HledÃ¡me span s role="list" kterÃ½ obsahuje rating stars
            rating_list = soup.find('span', {'role': 'list'})
            if rating_list:
                star_icons = rating_list.find_all('i', class_=re.compile(r'icon.*star', re.I))
                if star_icons and len(star_icons) <= 5:
                    info['stars'] = len(star_icons)
                    print(f"âœ… HodnocenÃ­ (z role=list): {info['stars']}â­")
                    stars_found = True
        
        # Fallback - hledÃ¡nÃ­ v textu
        if not stars_found:
            star_patterns = [
                {'class': re.compile(r'star', re.I)},
                {'class': re.compile(r'rating', re.I)}
            ]
            
            for pattern in star_patterns:
                stars_elem = soup.find(['span', 'div'], pattern)
                if stars_elem:
                    stars_text = stars_elem.get_text(strip=True)
                    stars_match = re.search(r'(\d)', stars_text)
                    if stars_match:
                        stars_num = int(stars_match.group(1))
                        if stars_num <= 5:  # Sanity check
                            info['stars'] = stars_num
                            print(f"âœ… HodnocenÃ­ (z textu): {info['stars']}â­")
                            break
        
        # ZÃ­skÃ¡nÃ­ textu strÃ¡nky pro dalÅ¡Ã­ zpracovÃ¡nÃ­
        page_text_lower = soup.get_text().lower()
        
        # HledÃ¡nÃ­ termÃ­nu (datum)
        date_patterns = [
            r'(\d{1,2}\.\d{1,2}\s*-\s*\d{1,2}\.\d{1,2}\.\d{4})',  # 12.05 - 15.05.2026
            r'termÃ­n[:\s]*(\d{1,2}\.\d{1,2}\s*-\s*\d{1,2}\.\d{1,2}\.\d{4})',
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, page_text_lower)
            if date_match:
                info['date'] = date_match.group(1).strip()
                print(f"âœ… TermÃ­n: {info['date']}")
                break
        
        # HledÃ¡nÃ­ poÄtu dnÃ­ - hledÃ¡me konkrÃ©tnÄ› vzor "X dny, Y noci" nebo "X dnÃ­"
        days_patterns = [
            r'\((\d+)\s+dn[yÃ­Å¯],?\s+\d+\s+noc[Ã­i]\)',  # (7 dnÃ­, 5 nocÃ­)
            r'(\d+)\s+dn[yÃ­Å¯],?\s+\d+\s+noc[Ã­i]',  # 7 dnÃ­, 5 nocÃ­
            r'(\d+)\s+dn[yÃ­Å¯]\b',  # 7 dny
            r'(\d+)\s+dnÅ¯\b',  # 7 dnÅ¯
            r'(\d+)\s+dnÃ­\b',  # 7 dnÃ­
        ]
        
        found_days = False
        for pattern in days_patterns:
            days_match = re.search(pattern, page_text_lower)
            if days_match:
                info['days'] = int(days_match.group(1))
                print(f"âœ… PoÄet dnÃ­: {info['days']}")
                found_days = True
                break
        
        # Pokud jsme nenaÅ¡li dny v textu, zkusÃ­me spoÄÃ­tat z termÃ­nu
        if not found_days and info['date']:
            try:
                # Parsujeme datum ve formÃ¡tu 09.09 - 15.09.2026
                date_parts = info['date'].replace(' ', '').split('-')
                if len(date_parts) == 2:
                    start_day = int(date_parts[0].split('.')[0])
                    end_parts = date_parts[1].split('.')
                    end_day = int(end_parts[0])
                    # VÃ½poÄet: konec - zaÄÃ¡tek + 1
                    info['days'] = end_day - start_day + 1
                    print(f"âœ… PoÄet dnÃ­ (vypoÄÃ­tÃ¡no z termÃ­nu): {info['days']}")
                    found_days = True
            except Exception as calc_error:
                print(f"âš ï¸  Chyba pÅ™i vÃ½poÄtu dnÃ­ z termÃ­nu: {calc_error}")
        
        # HledÃ¡nÃ­ stravy - vylepÅ¡enÃ© klÃ­ÄovÃ¡ slova
        meal_keywords = {
            'all inclusive': 'All Inclusive',
            'ultra all inclusive': 'Ultra All Inclusive',
            'polopenze': 'Polopenze',
            'plnÃ¡ penze': 'PlnÃ¡ penze', 
            'plna penze': 'PlnÃ¡ penze',
            'snÃ­danÄ›': 'SnÃ­danÄ›',
            'snidane': 'SnÃ­danÄ›',
            'bez stravy': 'Bez stravy',
            'pouze ubytovÃ¡nÃ­': 'Bez stravy',
            'light all inclusive': 'Light All Inclusive',
        }
        
        for keyword, meal_type in meal_keywords.items():
            if keyword in page_text_lower:
                info['meals'] = meal_type
                print(f"âœ… Strava: {info['meals']}")
                break
        
        # UpÅ™esnÄ›nÃ­ nÃ¡zvu hotelu ze strÃ¡nky
        title_tag = soup.find('h1')
        if title_tag:
            title_text = title_tag.get_text(strip=True)
            if 'hotel' in title_text.lower():
                info['hotel_name'] = title_text
                print(f"âœ… Hotel: {info['hotel_name']}")
        
    except requests.Timeout:
        print("âš ï¸  Timeout - strÃ¡nka se naÄÃ­tÃ¡ pÅ™Ã­liÅ¡ dlouho")
        print("â„¹ï¸  PouÅ¾iji zÃ¡kladnÃ­ data z URL...")
    except Exception as e:
        print(f"âš ï¸  Chyba pÅ™i scrapingu: {e}")
        print("â„¹ï¸  PouÅ¾iji zÃ¡kladnÃ­ data z URL...")
    
    return info

def generate_templates(info, short_url, affiliate_url):
    """Vygeneruje Å¡ablony pro kanÃ¡l a web"""
    
    # Pokud mÃ¡me termÃ­n, pÅ™idÃ¡me ho do Å¡ablony
    date_text = f" ({info['date']})" if info['date'] else ""
    
    channel_template = f"""Odkaz: {short_url}
ğŸŒ Last Minute ZÃ¡jezd
{info['flag']} {info['hotel_name']}, {info['destination']}
ğŸ“… Na {info['days']} dnÃ­{date_text}
âœˆï¸ Letenky a ğŸ¨ ubytovÃ¡nÃ­ ve {info['stars']}â­ï¸ hotelu
ğŸ½ï¸ Strava: {info['meals']}
ğŸ’° {info['price']}"""

    web_template = f"""{info['hotel_name']}
{info['flag']} {info['destination']}
ğŸ“… Na {info['days']} dnÃ­{date_text}
âœˆï¸ Letenky a ğŸ¨ ubytovÃ¡nÃ­ ve {info['stars']}â­ï¸ hotelu
ğŸ½ï¸ Strava: {info['meals']}
ğŸ’° {info['price']}
{affiliate_url}"""

    return channel_template, web_template

def main():
    print("=" * 60)
    print("ğŸš€ ÄŒEDOK URL KONVERTOR")
    print("=" * 60)
    print()
    
    # ZÃ­skÃ¡nÃ­ URL od uÅ¾ivatele
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = input("VloÅ¾te ÄŒedok URL: ").strip()
    
    if not url:
        print("âŒ Nebyla zadÃ¡na Å¾Ã¡dnÃ¡ URL!")
        sys.exit(1)
    
    print()
    print("âš™ï¸  ZpracovÃ¡vÃ¡m...")
    print()
    
    # 1. Scraping dat
    info = scrape_cedok_page(url)
    
    # 2. VytvoÅ™enÃ­ affiliate URL
    affiliate_url = f"{AFFILIATE_PREFIX}?url={urllib.parse.quote(url)}"
    print(f"âœ… Affiliate URL vytvoÅ™ena")
    
    # 3. ZkrÃ¡cenÃ­ pÅ™es Bitly
    print("ğŸ”— Zkracuji URL pÅ™es Bitly...")
    short_url = shorten_with_bitly(affiliate_url)
    print(f"âœ… ZkrÃ¡cenÃ¡ URL: {short_url}")
    
    # 4. GenerovÃ¡nÃ­ Å¡ablon
    channel_template, web_template = generate_templates(info, short_url, affiliate_url)
    
    print()
    print("=" * 60)
    print("ğŸ“± Å ABLONA NA KANÃL")
    print("=" * 60)
    print()
    print(channel_template)
    print()
    print("=" * 60)
    print("ğŸŒ Å ABLONA NA WEB")
    print("=" * 60)
    print()
    print(web_template)
    print()
    print("=" * 60)
    print("âœ… HOTOVO!")
    print("=" * 60)

if __name__ == "__main__":
    main()
